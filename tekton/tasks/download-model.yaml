apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: download-model
spec:
  description: |
    Deploy Kubernetes Gateway API Inference Extension.

  workspaces:
    - name: model-cache
      mountPath: /cache
    - name: hf-credentials
      readOnly: true

  params:
    - name: model
      type: string

  steps:
    - name: download-model
      image: python:3.10
      env:
        - name: HOME
          value: /tmp
        - name: HF_HOME
          value: /tmp/huggingface
        - name: MOUNT_PATH
          value: /cache # mount path of workspace model-cache
        - name: HF_MODEL_ID
          value: "$(params.model)"
        - name: MODEL_PATH
          value: "models/$(params.model)"
      script: |
        #!/bin/sh
        set -eu

        export HF_TOKEN=$(cat "$(workspaces.hf-credentials.path)/HF_TOKEN")
        export PATH="${PATH}:${HOME}/.local/bin"

        # Check if model already exists on PVC
        if [ -d "${MOUNT_PATH}/${MODEL_PATH}" ] && [ -n "$(ls -A ${MOUNT_PATH}/${MODEL_PATH} 2>/dev/null)" ]; then
          echo "Model already exists at ${MOUNT_PATH}/${MODEL_PATH}, skipping download"
          ls -la "${MOUNT_PATH}/${MODEL_PATH}" | head -20
          exit 0
        fi

        echo "Model not found, downloading ${HF_MODEL_ID}..."
        mkdir -p "${MOUNT_PATH}/${MODEL_PATH}"
        python -m pip install huggingface_hub
        hf auth login --token "${HF_TOKEN}"
        hf download "${HF_MODEL_ID}" --local-dir "/cache/${MODEL_PATH}"
